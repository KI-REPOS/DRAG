ğŸ“š Retrieval-Augmented Generation (RAG) Pipeline

This project implements a RAG pipeline using ChromaDB + Flask.
Users can upload documents (PDF, DOCX, PPT, TXT, PNG, JPEG) and URLs (websites, YouTube videos).
The system converts them into vector embeddings, stores them in ChromaDB, and allows interactive querying with an LLM.

<details> <summary>âœ¨ Features</summary>

ğŸ“‚ Ingest multiple document types (PDF, DOCX, PPT, TXT, PNG, JPEG)

ğŸŒ Accept websites & YouTube video links

ğŸ§© Automatic chunking + vector embeddings

ğŸ—„ï¸ Storage in ChromaDB

ğŸ¤– Query interface powered by Flask

âœ¨ Results refined with Phi-2 (quantized) model

ğŸ–¥ï¸ Clean web UI for interaction

</details>
<details> <summary>ğŸ› ï¸ Project Workflow</summary>

Add Documents â†’ Run ingest_data.py (processes input â†’ embeddings â†’ ChromaDB).

Query System â†’ Run app.py (Flask UI â†’ query â†’ retrieve â†’ LLM refinement).

Results â†’ See polished answers in the web interface.

</details>
<details> <summary>âš¡ Tech Stack</summary>

Backend â†’ Python, Flask

Vector DB â†’ ChromaDB

LLM â†’ Phi-2 Quantized (phi2Q_4_k.ggfu)

Embeddings â†’ Hugging Face Models

Frontend â†’ Flask + HTML Templates

</details>
<details> <summary>ğŸ“¥ Installation & Setup</summary>
1ï¸âƒ£ Clone the repo
git clone https://github.com/your-username/rag-pipeline.git
cd rag-pipeline

2ï¸âƒ£ Create & activate virtual environment
python -m venv venv
# Activate:
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

4ï¸âƒ£ Download Model

The Phi2Q_4_k.ggfu model is large, so itâ€™s not included in this repo.
ğŸ‘‰ Download Here
 and place it in the models/ directory.

</details>
<details> <summary>âš™ï¸ Running the Pipeline</summary>
Step 1: Ingest Data
python ingest_data.py


âœ”ï¸ Converts input files/links into chunks
âœ”ï¸ Generates embeddings
âœ”ï¸ Stores them in ChromaDB

Step 2: Run Flask App
python app.py


âœ”ï¸ Launches the web interface
âœ”ï¸ Accepts user queries
âœ”ï¸ Retrieves top-k relevant chunks
âœ”ï¸ Refines answers using Phi-2 model
âœ”ï¸ Displays results to user

</details>
<details> <summary>ğŸ“‚ Supported File & Link Types</summary>

PDF (.pdf)

Word (.docx)

PowerPoint (.ppt)

Text (.txt)

Images (.png, .jpeg)

Website URLs

YouTube video links

</details>
<details> <summary>ğŸ–¼ï¸ Project Flow Diagram</summary>
flowchart LR
    A[User Uploads Docs/Links] --> B[Chunking + Embeddings]
    B --> C[ChromaDB Storage]
    D[User Query] --> E[Retrieve Top-k Matches]
    E --> F[Phi-2 Model]
    F --> G[Polished Answer to User]

</details>
<details> <summary>ğŸ¯ Example Usage</summary>

Upload a PDF on quantum computing.

Run ingest_data.py â†’ indexed into ChromaDB.

Start app.py â†’ Ask: â€œExplain qubits in simple termsâ€.

System retrieves context + Phi-2 generates a polished answer.

</details>
<details> <summary>ğŸ“Œ Notes</summary>

Model is not included due to size (1.66 GB).

Use external download link provided.

Repo includes only code, configs, and instructions.

Add venv/ and large files to .gitignore.

</details>
<details> <summary>ğŸ¤ Contributing</summary>

Pull requests and feature suggestions are welcome!
Fork this repo â†’ make changes â†’ submit PR ğŸš€

</details>
<details> <summary>ğŸ“œ License</summary>

This project is licensed under the MIT License.

</details>
