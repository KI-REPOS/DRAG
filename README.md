<h1>ğŸ“š Retrieval-Augmented Generation (RAG) Pipeline</h1>
ğŸ”¥ Overview

This project implements a RAG pipeline using ChromaDB + Flask.
It lets users upload documents (PDF, DOCX, PPT, TXT, PNG, JPEG) and URLs (websites, YouTube videos), converts them into vector embeddings, stores them in ChromaDB, and allows interactive querying with an LLM.

âœ¨ Key Highlights

ğŸ§© Automatic Chunking + Embeddings

ğŸ“‚ Multi-format support â†’ PDF, DOCX, PPT, TXT, PNG, JPEG

ğŸŒ Websites & YouTube ingestion

ğŸ—„ï¸ Vector storage with ChromaDB

ğŸ¤– Phi-2 Quantized LLM (phi2Q_4_k.ggfu)

ğŸ¨ Flask Web UI for seamless interaction

<details> <summary><h2>âš¡ Project Workflow</h2></summary>
 <ul>
    <li>Option 1</li>
    <li>Option 2</li>
    <li>Option 3</li>
  </ul>
ğŸ”¹ Step 1: Ingest Data

Run ingest_data.py

Converts files/URLs â†’ chunks â†’ embeddings â†’ stored in ChromaDB

ğŸ”¹ Step 2: Query System

Run app.py

Query â†’ Retrieve top-k relevant chunks â†’ Refined via Phi-2 LLM â†’ Displayed in Flask UI

</details>
<details> <summary><h2>ğŸ› ï¸ Tech Stack</h2></summary>
ğŸ–¥ï¸ Backend

Python

Flask

ğŸ“¦ Database

ChromaDB (Vector DB)

ğŸ§  LLM

Phi-2 (Quantized phi2Q_4_k.ggfu)

ğŸ” Embeddings

Hugging Face Models

ğŸ¨ Frontend

Flask Templates (HTML, CSS)

</details>
<details> <summary><h2>ğŸ“¥ Installation & Setup</h2></summary>
1ï¸âƒ£ Clone Repo
git clone https://github.com/your-username/rag-pipeline.git
cd rag-pipeline

2ï¸âƒ£ Create & Activate Virtual Environment
python -m venv venv
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

4ï¸âƒ£ Download Model

The Phi2Q_4_k.ggfu model is large (1.66 GB), so itâ€™s not in this repo.
ğŸ‘‰ Download Here
 and place inside models/ directory.

</details>
<details> <summary><h2>âš™ï¸ Running the Pipeline</h2></summary>
â–¶ï¸ Ingest Data
python ingest_data.py

â–¶ï¸ Launch Web App
python app.py


âœ”ï¸ Opens Flask Web UI
âœ”ï¸ Accepts queries
âœ”ï¸ Retrieves top-k matches
âœ”ï¸ Refines with Phi-2 LLM
âœ”ï¸ Displays final result

</details>
<details> <summary><h2>ğŸ“‚ Supported File & Link Types</h2></summary>

ğŸ“„ PDF (.pdf)

ğŸ“ Word (.docx)

ğŸï¸ PowerPoint (.ppt)

ğŸ“œ Text (.txt)

ğŸ–¼ï¸ Images (.png, .jpeg)

ğŸŒ Website URLs

â–¶ï¸ YouTube video links

</details>
<details> <summary><h2>ğŸ–¼ï¸ Project Flow Diagram</h2></summary>
flowchart LR
    A[User Uploads Docs/Links] --> B[Chunking + Embeddings]
    B --> C[ChromaDB Storage]
    D[User Query] --> E[Retrieve Top-k Matches]
    E --> F[Phi-2 Model]
    F --> G[Polished Answer to User]

</details>
<details> <summary><h2>ğŸ¯ Example Usage</h2></summary>

Upload a PDF on quantum computing

Run ingest_data.py â†’ indexed into ChromaDB

Start app.py â†’ Ask: â€œExplain qubits in simple termsâ€

System retrieves context + Phi-2 generates a refined answer

</details>
<details> <summary><h2>ğŸ“Œ Notes</h2></summary>

ğŸš« Model not included (too large) â†’ use provided download link

ğŸ“¦ Repo only contains code, configs, instructions

ğŸ—‚ï¸ Use .gitignore â†’ exclude venv/ & large files

</details>
<details> <summary><h2>ğŸ¤ Contributing</h2></summary>

ğŸ’¡ Fork â†’ Create a feature branch â†’ Commit â†’ Push â†’ Submit PR ğŸš€

</details>
<details> <summary><h2>ğŸ“œ License</h2></summary>

ğŸ“„ Licensed under the MIT License

</details>
